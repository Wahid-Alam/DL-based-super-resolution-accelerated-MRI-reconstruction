{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f499f0",
   "metadata": {},
   "source": [
    "# Training the MoDL network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ece546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This is the training code to train the model as described in the following article:\n",
    "\n",
    "MoDL: Model-Based Deep Learning Architecture for Inverse Problems\n",
    "by H.K. Aggarwal, M.P. Mani, M. Jacob from University of Iowa.\n",
    "\n",
    "Paper dwonload  Link:     https://arxiv.org/abs/1712.02862\n",
    "\n",
    "This code solves the following optimization problem:\n",
    "\n",
    "    argmin_x ||Ax-b||_2^2 + ||x-Dw(x)||^2_2\n",
    "\n",
    " 'A' can be any measurement operator. Here we consider parallel imaging problem in MRI where\n",
    " the A operator consists of undersampling mask, FFT, and coil sensitivity maps.\n",
    "\n",
    "Dw(x): it represents the residual learning CNN.\n",
    "\n",
    "Here is the description of the parameters that you can modify below.\n",
    "\n",
    "epochs: how many times to pass through the entire dataset\n",
    "\n",
    "nLayer: number of layers of the convolutional neural network.\n",
    "        Each layer will have filters of size 3x3. There will be 64 such filters\n",
    "        Except at the first and the last layer.\n",
    "\n",
    "gradientMethod: MG or AG. set MG for 'manual gradient' of conjuagate gradient (CG) block\n",
    "                as discussed in section 3 of the above paper. Set it to AG if\n",
    "                you want to rely on the tensorflow to calculate gradient of CG.\n",
    "\n",
    "K: it represents the number of iterations of the alternating strategy as\n",
    "    described in Eq. 10 in the paper.  Also please see Fig. 1 in the above paper.\n",
    "    Higher value will require a lot of GPU memory. Set the maximum value to 20\n",
    "    for a GPU with 16 GB memory. Higher the value more is the time required in training.\n",
    "\n",
    "sigma: the standard deviation of Gaussian noise to be added in the k-space\n",
    "\n",
    "batchSize: You can reduce the batch size to 1 if the model does not fit on GPU.\n",
    "\n",
    "Output:\n",
    "\n",
    "After running the code the output model will be saved in the subdirectory 'savedModels'.\n",
    "You can give the name of the generated ouput directory in the tstDemo.py to\n",
    "run the newly trained model on the test data.\n",
    "\n",
    "\n",
    "@primary author: Hemant Kumar Aggarwal\n",
    "\"\"\"\n",
    "\n",
    "# import some libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    " \n",
    "import os,time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    " \n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import supportingFunctions as sf\n",
    "import model as mm\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#% SET THESE PARAMETERS CAREFULLY\n",
    "nLayers=5\n",
    "epochs=100\n",
    "batchSize=1\n",
    "gradientMethod='AG'\n",
    "K=10\n",
    "sigma=0.0001\n",
    "ncoils= 1\n",
    "nx=128\n",
    "ny=128\n",
    "restoreWeights=False\n",
    "\n",
    "#%% to train the model with higher K values  (K>1) such as K=5 or 10,\n",
    "#if K>1:\n",
    " #   restoreWeights=True\n",
    " #   restoreFromModel='26Oct_0117am_5L_1K_100E_AG'\n",
    "#if restoreWeights:\n",
    "#    wts=sf.getWeights('savedModels/'+restoreFromModel)\n",
    "#--------------------------------------------------------------------------\n",
    "#%%Generate a meaningful filename to save the trainined models for testing\n",
    "print ('*************************************************')\n",
    "start_time=time.time()\n",
    "saveDir='savedModels/'\n",
    "cwd=os.getcwd()\n",
    "directory=saveDir+datetime.now().strftime(\"%d%b_%I%M%P_\")+ \\\n",
    " str(nLayers)+'L_'+str(K)+'K_'+str(epochs)+'E_'+gradientMethod\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "sessFileName= directory+'/model'\n",
    "\n",
    "#%% save test model\n",
    "tf.reset_default_graph()\n",
    "csmT = tf.placeholder(tf.complex64,shape=(None,ncoils,nx,ny),name='csm')\n",
    "maskT= tf.placeholder(tf.complex64,shape=(None,nx,ny),name='mask')\n",
    "atbT = tf.placeholder(tf.float32,shape=(None,nx,ny,2),name='atb')\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,False,nLayers,K,gradientMethod)\n",
    "predTst=out['dc'+str(K)]\n",
    "predTst=tf.identity(predTst,name='predTst')\n",
    "sessFileNameTst=directory+'/modelTst'\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    savedFile=saver.save(sess, sessFileNameTst,latest_filename='checkpointTst')\n",
    "print ('testing model saved:' +savedFile)\n",
    "#%% read multi-channel dataset\n",
    "trnOrg,trnAtb,trnCsm,trnMask=sf.getData('training')\n",
    "trnOrg,trnAtb=sf.c2r(trnOrg),sf.c2r(trnAtb)\n",
    "\n",
    "#%%\n",
    "tf.reset_default_graph()\n",
    "csmP = tf.placeholder(tf.complex64,shape=(None,None,None,None),name='csm')\n",
    "maskP= tf.placeholder(tf.complex64,shape=(None,None,None),name='mask')\n",
    "atbP = tf.placeholder(tf.float32,shape=(None,None,None,2),name='atb')\n",
    "orgP = tf.placeholder(tf.float32,shape=(None,None,None,2),name='org')\n",
    "\n",
    "\n",
    "#%% creating the dataset\n",
    "nTrn=trnOrg.shape[0]\n",
    "nBatch= int(np.floor(np.float32(nTrn)/batchSize))\n",
    "nSteps= nBatch*epochs\n",
    "\n",
    "trnData = tf.data.Dataset.from_tensor_slices((orgP,atbP,csmP,maskP))\n",
    "trnData = trnData.cache()\n",
    "trnData=trnData.repeat(count=epochs)\n",
    "trnData = trnData.shuffle(buffer_size=trnOrg.shape[0])\n",
    "trnData=trnData.batch(batchSize)\n",
    "trnData=trnData.prefetch(5)\n",
    "iterator=trnData.make_initializable_iterator()\n",
    "orgT,atbT,csmT,maskT = iterator.get_next('getNext')\n",
    "\n",
    "#%% make training model\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,True,nLayers,K,gradientMethod)\n",
    "predT=out['dc'+str(K)]\n",
    "predT=tf.identity(predT,name='pred')\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.pow(predT-orgT, 2),axis=0))\n",
    "tf.summary.scalar('loss', loss)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    opToRun=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "#%% training code\n",
    "\n",
    "\n",
    "print ('training started at', datetime.now().strftime(\"%d-%b-%Y %I:%M %P\"))\n",
    "print ('parameters are: Epochs:',epochs,' BS:',batchSize,'nSteps:',nSteps,'nSamples:',nTrn)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "totalLoss,ep=[],0\n",
    "lossT = tf.placeholder(tf.float32)\n",
    "lossSumT = tf.summary.scalar(\"TrnLoss\", lossT)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if restoreWeights:\n",
    "        sess=sf.assignWts(sess,nLayers,wts)\n",
    "\n",
    "    feedDict={orgP:trnOrg,atbP:trnAtb, maskP:trnMask,csmP:trnCsm}\n",
    "    sess.run(iterator.initializer,feed_dict=feedDict)\n",
    "    savedFile=saver.save(sess, sessFileName)\n",
    "    print(\"Model meta graph saved in::%s\" % savedFile)\n",
    "\n",
    "    writer = tf.summary.FileWriter(directory, sess.graph)\n",
    "    for step in tqdm(range(nSteps)):\n",
    "        try:\n",
    "            tmp,_,_=sess.run([loss,update_ops,opToRun])\n",
    "            totalLoss.append(tmp)\n",
    "            if np.remainder(step+1,nBatch)==0:\n",
    "                ep=ep+1\n",
    "                avgTrnLoss=np.mean(totalLoss)\n",
    "                lossSum=sess.run(lossSumT,feed_dict={lossT:avgTrnLoss})\n",
    "                writer.add_summary(lossSum,ep)\n",
    "                totalLoss=[] #after each epoch empty the list of total loos\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    savedfile=saver.save(sess, sessFileName,global_step=ep,write_meta_graph=True)\n",
    "    writer.close()\n",
    "\n",
    "end_time = time.time()\n",
    "print ('Trianing completed in minutes ', ((end_time - start_time) / 60))\n",
    "print ('training completed at', datetime.now().strftime(\"%d-%b-%Y %I:%M %P\"))\n",
    "print ('*************************************************')\n",
    "\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a1b21",
   "metadata": {},
   "source": [
    "# Testing the network output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea2802",
   "metadata": {},
   "source": [
    "## selecting the saved models and loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b435d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import supportingFunctions as sf\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py as h5\n",
    "import mat73\n",
    "\n",
    "\n",
    "cwd=os.getcwd()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#%% choose a model from savedModels directory\n",
    "subDirectory= '05Apr_1151pm_5L_10K_50E_AG' # R=4 for sagittal slices Super-resolution training\n",
    " \n",
    "#%%Read the testing data from dataset.hdf5 file\n",
    "\n",
    "#tstOrg is the original ground truth\n",
    "#tstAtb: it is the aliased/noisy image\n",
    "#tstCsm: this is coil sensitivity maps\n",
    "#tstMask: it is the undersampling mask\n",
    "\n",
    "#tstOrg,tstAtb,tstCsm,tstMask=sf.getTestingData()\n",
    "#you can also read more testing data from dataset.hdf5 (see readme) file using the command\n",
    "\n",
    "filename= 'modl_dataset_SR_size64_testing.hdf5' # R=4 for sagittal slices supper-resolution testing\n",
    "\n",
    "with h5.File(filename) as f:\n",
    "    tstOrg,tstCsm,tstMask=f['tstOrg'][:],f['tstCsm'][:],f['tstMask'][:]\n",
    "\n",
    "atb=sf.generateUndersampled(tstOrg,tstCsm,tstMask,sigma=0.001)\n",
    "tstAtb=sf.c2r(atb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a56537b",
   "metadata": {},
   "source": [
    "## Loading the model and doing the slice by slice recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir= cwd+'/savedModels/'+subDirectory #complete path\n",
    "rec=np.empty(tstAtb.shape,dtype=np.complex64) #rec variable will have output\n",
    "\n",
    "tf.reset_default_graph()\n",
    "loadChkPoint=tf.train.latest_checkpoint(modelDir)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    new_saver = tf.train.import_meta_graph(modelDir+'/modelTst.meta')\n",
    "    new_saver.restore(sess, loadChkPoint)\n",
    "    graph = tf.get_default_graph()\n",
    "    predT =graph.get_tensor_by_name('predTst:0')\n",
    "    maskT =graph.get_tensor_by_name('mask:0')\n",
    "    atbT=graph.get_tensor_by_name('atb:0')\n",
    "    csmT   =graph.get_tensor_by_name('csm:0')\n",
    "    wts=sess.run(tf.global_variables())\n",
    "    start_time=time.time()\n",
    "    for i in range(len(tstAtb)):\n",
    "        atb,csm,mask=tstAtb[i],tstCsm[i],tstMask[i]\n",
    "        na=np.newaxis\n",
    "        dataDict={atbT:atb[na],maskT:mask[na] ,csmT:csm[na] }\n",
    "        rec[i]=sess.run(predT,feed_dict=dataDict)\n",
    "end_time=time.time()\n",
    "rec=sf.r2c(rec.squeeze())\n",
    "print ('Trianing completed in seconds ', ((end_time - start_time)))\n",
    "print('Reconstruction done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02777727",
   "metadata": {},
   "source": [
    "## Quantitative evaluation w.r.t. ground truth and Visualization for a representative slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=17\n",
    "psnrAtb=sf.myPSNR(normAtb[num],normOrg[num])\n",
    "psnrRec=sf.myPSNR(normRec[num],normOrg[num])\n",
    "\n",
    "print ('*****************')\n",
    "print ('  ' + 'Org-Atb ' + 'Org-Rec')\n",
    "print ('  {0:.2f}    {1:.2f}'.format(psnrAtb,psnrRec))\n",
    "#print ('{0:.2f}'.format(psnrRec))\n",
    "print ('*****************')\n",
    "plot= lambda x: plt.imshow(x,cmap=plt.cm.gray, clim=(0.0, 0.8))\n",
    "plt.clf()\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.subplot(141)\n",
    "plot(np.fft.fftshift(tstMask[num]))\n",
    "#plot(tstMask[0])\n",
    "plt.axis('off')\n",
    "plt.title('Mask')\n",
    "plt.subplot(142)\n",
    "plot(np.fliplr(normOrg[num]))\n",
    "#plot(np.fft.ifftshift(normOrg[num], axes=0))\n",
    "plt.axis('off')\n",
    "plt.title('ground-truth')\n",
    "plt.subplot(143)\n",
    "plot(np.fliplr(normAtb[num]))\n",
    "#plot(np.fft.ifftshift(normAtb[:,:,num], axes=0))\n",
    "plt.title('input \\n PSNR='+ str(psnrAtb.round(2)) +' dB')\n",
    "#plt.title('recon from Bart')\n",
    "plt.axis('off')\n",
    "plt.subplot(144)\n",
    "plot(normRec[num])\n",
    "#plot(np.fft.ifftshift(normRec[:,:,num], axes=0))\n",
    "#plt.title('MoDL, PSNR='+ str(psnrRec.round(2)) +' dB')\n",
    "plt.title('output \\n PSNR='+ str(psnrRec.round(2)) +' dB')\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0,wspace=.01)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
